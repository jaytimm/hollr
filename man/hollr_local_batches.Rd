% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hollr_local_batches.R
\name{hollr_local_batches}
\alias{hollr_local_batches}
\title{Batch Processing for Local LLM}
\usage{
hollr_local_batches(
  id,
  user_message = "",
  annotators = 1,
  model,
  temperature = 1,
  top_p = 1,
  max_new_tokens = 100,
  max_length = NULL,
  system_message = "",
  batch_size = 10
)
}
\arguments{
\item{id}{A unique identifier for the request.}

\item{user_message}{The message provided by the user.}

\item{annotators}{The number of annotators (default is 1).}

\item{model}{The name of the model to use.}

\item{temperature}{The temperature for the model's output (default is 1).}

\item{top_p}{The top-p sampling value (default is 1).}

\item{max_new_tokens}{The maximum number of new tokens to generate (default is 100).}

\item{max_length}{The maximum length of the input prompt (default is NULL).}

\item{system_message}{The message provided by the system (default is '').}

\item{batch_size}{The number of messages to process in each batch (default is 10).}
}
\value{
A list containing the generated responses for each batch.
}
\description{
This function generates text in batches using a local model.
}
\examples{
\dontrun{
hollr_local_batches(id = "example_id", 
user_message = "Hello, how are you?", 
model = "local_model")
}
}
